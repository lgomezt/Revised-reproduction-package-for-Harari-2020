# Cargamos los paquetes necesarios
rm(list = ls())
library(haven) # To load .dta files in R
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_script
gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "/Data/")
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
path_output
# Cargamos los paquetes necesarios
rm(list = ls())
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Filter the data to select only Kolkata (id = 457) and Bangalore (id = 150)
# for the year 2005
df <- df %>%
filter(year == 2005) %>%
filter(id == 457 | id == 150) %>%
select(spin_km, range_km, remoteness_km, disconnect_km, spin_N_km,
range_N_km, remoteness_N_km, disconnect_N_km, area_polyg_km, id)
# Create new variable called NoN_EACradius that represents the radious of a circle
# If Area = pi * radio^2 => radio = (Area/pi)^0.5
df$NoN_EACradius = sqrt(df$area_polyg_km/pi) # pi~ 3.141593
df$Norm_EACradius = df$NoN_EACradius #
# The autor puts the suffix NoN or Norm in the name of the variables to declare
# that it is Not Normalized (NoN) or it is Normalized (Norm). Therefore, she
# changes the name of some variables:
names(df) <- c("NoN_spin", "NoN_range", "NoN_remoteness", "NoN_disconnect",
"Norm_spin", "Norm_range", "Norm_remoteness", "Norm_disconnect",
"area_polyg_km", "id", "NoN_EACradius", "Norm_EACradius")
# Drop area_polyg_km
df <- select(df, -area_polyg_km)
# Reshape from wide to long
df <- df %>%
pivot_longer(-id, names_to = c("type", "Shape metric"), names_sep = "_") %>%
pivot_wider(id_cols = c("id", "Shape metric"), names_from = "type")
# Put the city name and drop id
df <- df %>%
mutate(city = ifelse(id == 150, "B", "K")) %>%
select(-id)
# Reshape from long to wide
df <- df %>%
pivot_wider(id_cols = "Shape metric", names_from = "city",
values_from = c("NoN", "Norm"))
# Create the variable NoN_rescale_B as Norm_B * K_area
K_area = df$NoN_K[df$`Shape metric` == "EACradius"]
df$NoN_rescale_B = df$Norm_B * K_area
# Create the variables Adj_Diff and NoAdj_Diff
df$Adj_Diff = df$NoN_K - df$NoN_rescale_B
df$NoAdj_Diff = df$NoN_K - df$NoN_B
# Drop EACradius metric
df <- df[df$`Shape metric` != "EACradius",]
# Change the names of the shape metrics and the columns to export the file
df <- select(df, c(`Shape metric`, NoN_K, Norm_K, NoN_rescale_B, Norm_B))
df$`Shape metric` <- c("3. Spin, km2", "4. Range, km", "2. Remoteness, km",
"1. Disconnection, km")
names(df) <-  c("Shape metric", "NonNormMetric_Kolkata", "Normalized_Kolkata",
"NonNormMetric_Bangalore", "Normalized_Bangalore")
df <- arrange(df, `Shape metric`)
df
library(knitr)
# Quitar los warnings de todo el documento
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
kable(df)
kable(df, caption = "Figure 1. Shape metrics: An example")
rm(list = ls())
# Load libraries
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
library(tibble) # Rownames to columns
library(stargazer) # Export to latex
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Panel A. 1950, 1992â€“2010
# We show the statistic for the 351 cities sampled by the author for their
# main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# We create a function to count the number of rows that are not NA
not.na = function(x, na.rm = T) {sum(!is.na(x), na.rm)}
# Calculate summary statistics for area_polyg_km, disconnect_km,
# r1_relev_disconnect_cls_km
stats <- df2 %>%
select(area_polyg_km, disconnect_km, r1_relev_disconnect_cls_km) %>%
summarise_all(.funs = c("not.na", "mean", "median", "sd", "min", "max"), na.rm = T) %>%
pivot_longer(everything()) %>%
extract(name, into = c("variable", "stats"), "(.*)_([^_]+)$") %>%
pivot_wider(names_from = "variable")
# Calculate summary statistics for TOTAL_pop_all.
# We only take in account those cities that are in our sample. Therefore, we drop
# the cities that doesn't have information for log_area_polyg_km, id, year,
# disconnect_km
stats2 <- df2 %>%
drop_na(log_TOTAL_pop_all, log_area_polyg_km, id, year, disconnect_km) %>%
select(TOTAL_pop_all) %>%
summarise_all(.funs = c("not.na", "mean", "median", "sd", "min", "max"), na.rm = T)
# Append stats and stats2 and prettify to create the latex output
stats2$variable = "City population"
stats <- data.frame(t(stats))
stats <- rownames_to_column(stats, "variable")
stats[1, 1] <- "variable"
names(stats) <- stats[1,]
stats <- stats[-1,]
stats3 <- rbind(stats, stats2)
# Rename columns and variables
names(stats3) <- c("Variables", "Observations", "Mean", "Median", "SD", "Min", "Max")
stats3$Variables <- c("Area, km2", "Shape, km", "Potential shape, km",
"City population")
# Round decimals and put every number pretty
stats3$Observations <- as.integer(stats3$Observations)
stats3[1:3, c("Mean", "Median", "SD", "Min", "Max")] <- apply(
stats3[1:3, c("Mean", "Median", "SD", "Min", "Max")], 2,
function(x) round(as.numeric(x), 2))
stats3[4, c("Mean", "Median", "SD", "Min", "Max")] <- apply(
stats3[4, c("Mean", "Median", "SD", "Min", "Max")], 2,
function(x) round(as.numeric(x), 0))
stats3[1, 7] <- as.integer(stats3[1, 7])
stats3
kable(stats3, caption = "Table 1. Descriptive statistics. Panel A. 1950, 1992 - 2010", digits = 2, format.args = list(big.mark = ","))
apply(stats3, 2, function(x) prettyNum(x, big.mark = ","))
kable(stats3, caption = "Table 1. Descriptive statistics. Panel A. 1950, 1992 - 2010", digits = 2) %>%
kable_styling(
font_size = 15,
bootstrap_options = c("striped", "hover", "condensed")
)
kable(stats3, caption = "Table 1. Descriptive statistics. Panel A. 1950, 1992 - 2010", digits = 2)
kable(stats3, caption = "Table 1. Descriptive statistics. Panel B. Long difference", digits = 2)
# Delete all variables. Is the same as "clear all" in Stata
rm(list = ls())
# Load libraries
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
library(tibble) # Rownames to columns
library(stargazer) # Export to latex
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Panel B.
# We show the statistic for the 351 cities sampled by the author for their
# main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# We are interested in show de difference between 2010 and 1950. Therefore, we
# are going to concentrate in the mean of each year
stats <- df2 %>%
group_by(year) %>%
summarise(across(c(area_polyg_km, disconnect_km, r1_relev_disconnect_cls_km,
TOTAL_pop_all), list("mean" = ~mean(.x, na.rm = T),
"sd" = ~sd(.x, na.rm = T)))) %>%
filter(year %in% c(1950, 2010)) %>%
pivot_longer(-year) %>%
extract(name, into = c("variable", "stats"), "(.*)_([^_]+)$") %>%
pivot_wider(names_from = "variable")
# Now, we are going to calculate the difference between 2010 and 1950
stats2 <- df2 %>%
group_by(id) %>%
filter(year %in% c(1950, 2010)) %>%
summarise(across(c(area_polyg_km, disconnect_km, r1_relev_disconnect_cls_km,
TOTAL_pop_all), list("diff" = ~diff(.x, na.rm = T)))) %>%
summarise(across(c(area_polyg_km_diff, disconnect_km_diff, r1_relev_disconnect_cls_km_diff,
TOTAL_pop_all_diff), list("mean" = ~mean(.x, na.rm = T),
"sd" = ~sd(.x, na.rm = T)))) %>%
pivot_longer(everything()) %>%
extract(name, into = c("variable", "stats"), "(.*)_([^_]+)$") %>%
pivot_wider(names_from = "variable")
# Append two tables
names(stats2) <- names(stats)[-1]
stats2$year <- "2010-1950"
stats3 <- rbind(stats, stats2)
# Done! Let's arrange the format and export to Latex
stats3 <- data.frame(t(stats3))
stats3 <- rownames_to_column(stats3, "variable")
stats3[1, 1] <- "variable"
names(stats3) <- paste(stats3[1,], stats3[2,], sep = "_")
stats3 <- stats3[-c(1:2),]
stats3$variable_stats <- c("Area, km2", "Shape, km", "Potential shape, km",
"City population")
stats3 <- stats3 %>%
pivot_longer(cols = -variable_stats, names_sep = "_",
names_to = c("year", "stat")) %>%
pivot_wider(names_from = "year")
# Round decimals and put every number pretty
stats3[, 3:5] <- apply(stats3[, 3:5], 2, function(x) round(as.numeric(x), 2))
stats3[7:8, 3:5] <- apply(stats3[7:8, 3:5], 2, function(x) as.integer(x))
names(stats3)[1:2] <- c("Variables", "Statistic")
stats3 <- apply(stats3, 2, function(x) prettyNum(x, big.mark = ","))
kable(stats3, caption = "Table 1. Descriptive statistics. Panel B. Long difference", digits = 2)
# Delete all variables. Is the same as "clear all" in Stata
rm(list = ls())
# Load libraries
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
library(tibble) # Rownames to columns
library(stargazer) # Export to latex
library(lmtest) # Cluster standard errors
library(sandwich) # Robust covariance matrix estimators
library(AER) # To perform IV estimation
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Filter for the 351 cities sampled by the author for their main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# Reshape the data: long to wide
df2 <- df2 %>%
select(id, year, area_polyg_km, disconnect_N_km, disconnect_km,
log_projected_pop, r1_relev_disconnect_cls_km, log_area_polyg_km,
log_TOTAL_pop_all, dens_core_all, TOTAL_pop_all) %>%
pivot_wider(id_cols = id, names_from = year,
values_from = -all_of(c("id", "year")))
# List of variables to calculate the difference
variables <- c("area_polyg_km", "disconnect_N_km", "disconnect_km",
"log_projected_pop", "r1_relev_disconnect_cls_km",
"log_area_polyg_km", "log_TOTAL_pop_all", "dens_core_all",
"TOTAL_pop_all")
for (var in variables) {
# Names of the variables
var2010 <- paste(var, "2010", sep = "_")
var1950 <- paste(var, "1950", sep = "_")
vardiff <- paste(var, "diff", sep = "_")
# Calculation
df2[, vardiff] = df2[, var2010] - df2[, var1950]
}
f1_shape <- lm(disconnect_km_diff ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
f1_shape2 <- coeftest(f1_shape, vcov. = vcovHC(f1_shape, type = "HC0"),
cluster = id)
# First stage for Area (diff)
f1_area <- lm(log_area_polyg_km_diff ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
f1_area2 <- coeftest(f1_area, vcov. = vcovHC(f1_area, type = "HC0"),
cluster = id)
# Export results
stargazer(f1_shape2, f1_area2, type = "text", title = "Table 2: First stage",
column.labels = c("OLS", "OLS"),
covariate.labels = c("D Potential shape, km", "D Log projected population"),
dep.var.labels = c("Disconnection D2010-1950", "Log area D2010-1950"),
omit = c("Constant"), notes = "Robust standard errors in parentheses",
add.lines = list(c("Observations", nrow(df2), nrow(df2))))
f1_shape2
# Export results
stargazer(f1_shape2, f1_area2, type = "html", title = "Table 2: First stage",
column.labels = c("OLS", "OLS"),
covariate.labels = c("D Potential shape, km", "D Log projected population"),
dep.var.labels = c("Disconnection D2010-1950", "Log area D2010-1950"),
omit = c("Constant"), notes = "Robust standard errors in parentheses",
add.lines = list(c("Observations", nrow(df2), nrow(df2))))
# Delete all variables. Is the same as "clear all" in Stata
rm(list = ls())
# Load libraries
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
library(tibble) # Rownames to columns
library(stargazer) # Export to latex
library(lmtest) # Cluster standard errors
library(sandwich) # Robust covariance matrix estimators
library(AER) # To perform IV estimation
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Filter for the 351 cities sampled by the author for their main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# Reshape the data: long to wide
df2 <- df2 %>%
select(id, year, area_polyg_km, disconnect_N_km, disconnect_km,
log_projected_pop, r1_relev_disconnect_cls_km, log_area_polyg_km,
log_TOTAL_pop_all, dens_core_all, TOTAL_pop_all) %>%
pivot_wider(id_cols = id, names_from = year,
values_from = -all_of(c("id", "year")))
# List of variables to calculate the difference
variables <- c("area_polyg_km", "disconnect_N_km", "disconnect_km",
"log_projected_pop", "r1_relev_disconnect_cls_km",
"log_area_polyg_km", "log_TOTAL_pop_all", "dens_core_all",
"TOTAL_pop_all")
for (var in variables) {
# Names of the variables
var2010 <- paste(var, "2010", sep = "_")
var1950 <- paste(var, "1950", sep = "_")
vardiff <- paste(var, "diff", sep = "_")
# Calculation
df2[, vardiff] = df2[, var2010] - df2[, var1950]
}
# Why the author are using IV? What are the sources of endogeneity?
"""
# IV Estimates
iv <- ivreg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, instruments = ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
iv2 <- coeftest(iv, vcov. = vcovHC(iv, type = "HC0"),
iv2 <- coeftest(iv, vcov. = vcovHC(iv, type = "HC0"), cluster = id)
# IV Estimates
iv <- ivreg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, instruments = ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
iv2 <- coeftest(iv, vcov. = vcovHC(iv, type = "HC0"), cluster = id)
# Delete all variables. Is the same as "clear all" in Stata
rm(list = ls())
# Load libraries
library(haven) # To load .dta files in R
library(rstudioapi) # To get the directory of this script
library(dplyr) # To manipulate dataframes
library(tidyr) # To reshape dataframes
library(writexl) # To export dataframes to Excel
library(tibble) # Rownames to columns
library(stargazer) # Export to latex
library(lmtest) # Cluster standard errors
library(sandwich) # Robust covariance matrix estimators
library(AER) # To perform IV estimation
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Filter for the 351 cities sampled by the author for their main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# Reshape the data: long to wide
df2 <- df2 %>%
select(id, year, area_polyg_km, disconnect_N_km, disconnect_km,
log_projected_pop, r1_relev_disconnect_cls_km, log_area_polyg_km,
log_TOTAL_pop_all, dens_core_all, TOTAL_pop_all) %>%
pivot_wider(id_cols = id, names_from = year,
values_from = -all_of(c("id", "year")))
# List of variables to calculate the difference
variables <- c("area_polyg_km", "disconnect_N_km", "disconnect_km",
"log_projected_pop", "r1_relev_disconnect_cls_km",
"log_area_polyg_km", "log_TOTAL_pop_all", "dens_core_all",
"TOTAL_pop_all")
for (var in variables) {
# Names of the variables
var2010 <- paste(var, "2010", sep = "_")
var1950 <- paste(var, "1950", sep = "_")
vardiff <- paste(var, "diff", sep = "_")
# Calculation
df2[, vardiff] = df2[, var2010] - df2[, var1950]
}
# IV Estimates
iv <- ivreg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, instruments = ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
iv2 <- coeftest(iv, vcov. = vcovHC(iv, type = "HC0"), cluster = id)
# Interpretation of the results
"""
The betas of the IV regression show that as city becomes less compact (holding
area constant), the population growth declines. An increase in the average distance
between points of 360 meters (one-standar deviation increase), holding constant
city area, is associated with a 3.5 percent decline in population.
"""
# OLS Estimates
ols <- reg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, data = df2)
ols2 <- coeftest(ols, vcov. = vcovHC(ols, type = "HC0"),
cluster = id)
# Interpretation of the results
"""
In equilibrium, faster growing cities are cities that grow into more disconnected
shapes. A 1% increase in population is associated with a deterioration in shape of
450 meters. Potential channels of this effect are more difficult urban planning or
governance, urban growth occurring along transit corridors, and the tendency of
cities to expand into less favorable terrain.
"""
# Export results
stargazer(iv2, ols2, type = "latex",
title = "Table 3: Impact of city shape on population",
column.labels = c("IV", "OLS"),
covariate.labels = c("D Shape, km", "D Log area"),
dep.var.labels = c("Log Pop D2010-1950", "Log Pop D2010-1950"),
omit = c("Constant"),
notes = "Robust standard errors in parentheses",
add.lines = list(c("Observations", ols$n, ols$n)))
ols2 <- coeftest(ols, vcov. = vcovHC(ols, type = "HC0"), cluster = id)
# OLS Estimates
ols <- reg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, data = df2)
# Define the paths to import data, call scripts or save outputs
path_script <- getActiveDocumentContext()$path # Automatic path :D
path_data <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Data/")
path_output <- gsub(x = path_script,
pattern = "Entrega 4.Rmd",
replacement = "Out/")
# Import CityShape_Main.dta as df
df <- read_dta(file = paste0(path_data, "CityShape_Main.dta"))
# Filter for the 351 cities sampled by the author for their main analysis.
df2 = df[df["insample_IV_5010"] == 1,]
# Reshape the data: long to wide
df2 <- df2 %>%
select(id, year, area_polyg_km, disconnect_N_km, disconnect_km,
log_projected_pop, r1_relev_disconnect_cls_km, log_area_polyg_km,
log_TOTAL_pop_all, dens_core_all, TOTAL_pop_all) %>%
pivot_wider(id_cols = id, names_from = year,
values_from = -all_of(c("id", "year")))
# Generate log difference vars
# List of variables to calculate the difference
variables <- c("area_polyg_km", "disconnect_N_km", "disconnect_km",
"log_projected_pop", "r1_relev_disconnect_cls_km",
"log_area_polyg_km", "log_TOTAL_pop_all", "dens_core_all",
"TOTAL_pop_all")
for (var in variables) {
# Names of the variables
var2010 <- paste(var, "2010", sep = "_")
var1950 <- paste(var, "1950", sep = "_")
vardiff <- paste(var, "diff", sep = "_")
# Calculation
df2[, vardiff] = df2[, var2010] - df2[, var1950]
}
# IV Estimates
iv <- ivreg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff +
log_area_polyg_km_diff, instruments = ~ r1_relev_disconnect_cls_km_diff +
log_projected_pop_diff, data = df2)
iv2 <- coeftest(iv, vcov. = vcovHC(iv, type = "HC0"), cluster = id)
# Interpretation of the results
"""
The betas of the IV regression show that as city becomes less compact (holding
area constant), the population growth declines. An increase in the average distance
between points of 360 meters (one-standar deviation increase), holding constant
city area, is associated with a 3.5 percent decline in population.
"""
# IV Estimates
iv <- ivreg(formula = log_TOTAL_pop_all_diff ~ disconnect_km_diff + log_area_polyg_km_diff, instruments = ~ r1_relev_disconnect_cls_km_diff + log_projected_pop_diff, data = df2)
